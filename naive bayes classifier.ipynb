{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ckiptagger import WS, POS, NER, construct_dictionary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import re\n",
    "import pycantonese as pc\n",
    "import string\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Ckip Tagger Dict of Word and POS/NER tags\n",
    "ws = WS(\"./ckip_dict\")\n",
    "pos = POS(\"./ckip_dict\")\n",
    "ner = NER(\"./ckip_dict\")\n",
    "\n",
    "#Cantonese Dict\n",
    "file = open(\"./dict/cantonese_dict_converted_no_english_in_pydict.txt\", \"r\")\n",
    "Cantonese_Dict = eval(file.read())\n",
    "Cantonese_Dict = construct_dictionary(Cantonese_Dict)\n",
    "file.close()\n",
    "\n",
    "#Stopwords(Chinese Punctuation)\n",
    "file = open(\"./stopwords/punctuation_symbol.txt\", \"r\")\n",
    "punctuation_ch = file.read().split('\\n')\n",
    "file.close()\n",
    "\n",
    "#Stopwords(Written Chinese)\n",
    "file = open(\"./stopwords/written.txt\", \"r\")\n",
    "stopwords_written = file.read().split('\\n')\n",
    "file.close()\n",
    "\n",
    "#Stopwords(Cantonese)(Small set)\n",
    "stop_words_cantonese = pc.stop_words(add=[\"俾\",\"哋\",\"地\",\"埋\"])\n",
    "stop_words_cantonese = stop_words_cantonese.difference({\"冇\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess of whole string(remove useless string)(cofact dataset cleaning)\n",
    "def remove_bad_string(sentence):\n",
    "    #dataset cleaning\n",
    "    sentence = re.sub(r\"Upload Date & Time\\n.*發布時間.*\\n.*\\n.*\",\"\",sentence)\n",
    "    sentence = re.sub(r\"Add Friend.*Add LINE Friends via QR Code\\n.*\\n\\n.*\\n.*\",\"\",sentence)\n",
    "    sentence = re.sub(r\"get data from URL\",\"\",sentence)\n",
    "    sentence = re.sub(r\"Looked EverywhereFor This Page! (Error 404)\",\"\",sentence)\n",
    "    sentence = re.sub(r\"But maybe we can still help you findwhat you're looking for\",\"\",sentence)\n",
    "    \n",
    "    # remove old style retweet text \"RT\"\n",
    "    sentence = re.sub(r'^RT[\\s]+', '', sentence)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    sentence = re.sub(r'((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?', '', sentence)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    #sentence = re.sub(r'#', '', sentence)\n",
    "    \n",
    "    #standardize space\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    #1,600 -> 1600(\",\" often used for this way)\n",
    "    sentence = re.sub(r',', '', sentence)\n",
    "    \n",
    "    #remove emojify\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    sentence = regrex_pattern.sub(r'',sentence)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "#tokenization for Chinese\n",
    "def tokenize(topics, use_can_dict=True):\n",
    "    if use_can_dict:\n",
    "        word_sentence_list = ws(topics, sentence_segmentation = True, \n",
    "                                segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"},\n",
    "                                recommend_dictionary = Cantonese_Dict)\n",
    "    else:\n",
    "        word_sentence_list = ws(topics, sentence_segmentation = True, \n",
    "                                segment_delimiter_set = {\",\", \"。\", \":\", \"?\", \"!\", \";\"})\n",
    "    \n",
    "    return word_sentence_list\n",
    "\n",
    "#Preprocess for token(Stopwords/Punctuations)\n",
    "def filter_stopword(sentence):\n",
    "    \n",
    "    filter_word=[]\n",
    "    filter_list=[stopwords_written, stop_words_cantonese, string.punctuation, punctuation_ch]\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word == \" \":\n",
    "            continue\n",
    "        for filter_words in filter_list:\n",
    "            if word in filter_words:\n",
    "                break\n",
    "        else:\n",
    "            filter_word.append(word)\n",
    "    \n",
    "    return filter_word\n",
    "\n",
    "#Find POS of word segment\n",
    "#https://github.com/ckiplab/ckiptagger/wiki/POS-Tags\n",
    "def pos_tag(topic):\n",
    "    return pos([topic])[0]\n",
    "\n",
    "#Find Entity(Name/place/Time wtc.) in word segment\n",
    "#Name Entity Recognition\n",
    "#https://github.com/ckiplab/ckiptagger/wiki/Entity-Types\n",
    "def ner_tag(topic, ps):\n",
    "    return ner([topic], [ps])[0]\n",
    "\n",
    "#Word Segmentation\n",
    "def segment(topics, use_can_dict=True):\n",
    "    \n",
    "    for i in range(len(topics)):\n",
    "        topics[i] = remove_bad_string(topics[i])\n",
    "    topics = tokenize(topics, use_can_dict)\n",
    "    for i in range(len(topics)):\n",
    "        topics[i] = filter_stopword(topics[i])\n",
    "    \n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Dict that count the word freqs in/not in that category\n",
    "#format: {(word(str), in_that_cate(bool)): number(int)}\n",
    "#e.g. {(\"中國\", 1): 50, (\"中國\", 0): 25}\n",
    "def count_tweets(result, tweets, ys):\n",
    "    for y, tweet in zip(ys, tweets):\n",
    "        for word in tweet:\n",
    "            # define the key, which is the word and label tuple\n",
    "            pair = (word,y)\n",
    "            \n",
    "            result[pair] = result.get(pair,0) + 1\n",
    "\n",
    "    return result\n",
    "\n",
    "#Train Naive Bayes Model\n",
    "#return \n",
    "#logprior: (log prob. of text is that cate)\n",
    "#loglikelihood (of each word): (log prob. of text belong to cate given that word)\n",
    "def train_naive_bayes(freqs, train_x, train_y):\n",
    "    \n",
    "    loglikelihood = {}\n",
    "    logprior = 0\n",
    "\n",
    "    # calculate V, the number of unique words in the vocabulary\n",
    "    vocab = set([pair[0] for pair in freqs.keys()])\n",
    "    V = len(vocab)\n",
    "\n",
    "    # calculate N_pos, N_neg, V_pos, V_neg\n",
    "    N_pos = N_neg = V_pos = V_neg = 0\n",
    "    for pair in freqs.keys():\n",
    "        # if the label is positive (greater than zero)\n",
    "        if pair[1] > 0:\n",
    "            # increment the count of unique positive words by 1\n",
    "            V_pos += 1\n",
    "\n",
    "            # Increment the number of positive words by the count for this (word, label) pair\n",
    "            N_pos += freqs[pair]\n",
    "\n",
    "        # else, the label is negative\n",
    "        else:\n",
    "            # increment the count of unique negative words by 1\n",
    "            V_neg += 1\n",
    "\n",
    "            # increment the number of negative words by the count for this (word,label) pair\n",
    "            N_neg += freqs[pair]\n",
    "\n",
    "    # Calculate D, the number of documents\n",
    "    D = len(train_y)\n",
    "\n",
    "    # Calculate D_pos, the number of positive documents\n",
    "    D_pos = len(list(filter(lambda x: x==1, train_y)))\n",
    "\n",
    "    # Calculate D_neg, the number of negative documents\n",
    "    D_neg = len(list(filter(lambda x: x==0, train_y)))\n",
    "\n",
    "    # Calculate logprior\n",
    "    logprior = np.log(D_pos) - np.log(D_neg)\n",
    "\n",
    "    # For each word in the vocabulary...\n",
    "    for word in vocab:\n",
    "        # get the positive and negative frequency of the word\n",
    "        freq_pos = freqs.get((word,1),0)\n",
    "        freq_neg = freqs.get((word,0),0)\n",
    "\n",
    "        # calculate the probability that each word is positive, and negative\n",
    "        p_w_pos = (freq_pos+1)/(N_pos+V)\n",
    "        p_w_neg = (freq_neg+1)/(N_neg+V)\n",
    "\n",
    "        # calculate the log likelihood of the word\n",
    "        loglikelihood[word] = np.log(p_w_pos) -np.log(p_w_neg)\n",
    "\n",
    "    return logprior, loglikelihood\n",
    "\n",
    "#Prediction of Naive Bayes Model\n",
    "#The score(prob.) of text belong to that cate\n",
    "#Normally, if score > 0 mean that text belong to that cate\n",
    "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
    "    \n",
    "    # process the tweet to get a list of words\n",
    "    word_l = tweet\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "\n",
    "    return p\n",
    "\n",
    "#Same as above\n",
    "#But also print the score of each word\n",
    "def naive_bayes_predict_detail(word_l, logprior, loglikelihood):\n",
    "\n",
    "    # initialize probability to zero\n",
    "    p = 0\n",
    "\n",
    "    # add the logprior\n",
    "    p += logprior\n",
    "    print(f\"Prior: {logprior}\")\n",
    "\n",
    "    for word in word_l:\n",
    "\n",
    "        # check if the word exists in the loglikelihood dictionary\n",
    "        if word in loglikelihood:\n",
    "            print(f\"{word}: {loglikelihood[word]}\")\n",
    "            # add the log likelihood of that word to the probability\n",
    "            p += loglikelihood[word]\n",
    "        else:\n",
    "            print(f\"{word}: {0}\")\n",
    "\n",
    "    return p\n",
    "\n",
    "#Testing of Model\n",
    "#(Feed in testing data to see the accuracy)\n",
    "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
    "    accuracy = 0  # return this properly\n",
    "    \n",
    "    y_hats = []\n",
    "    for tweet in test_x:\n",
    "        # if the prediction is > 0\n",
    "        if naive_bayes_predict(tweet, logprior, loglikelihood) > 0:\n",
    "            # the predicted class is 1\n",
    "            y_hat_i = 1\n",
    "        else:\n",
    "            # otherwise the predicted class is 0\n",
    "            y_hat_i = 0\n",
    "\n",
    "        # append the predicted class to the list y_hats\n",
    "        y_hats.append(y_hat_i)\n",
    "\n",
    "    # error is the average of the absolute values of the differences between y_hats and test_y\n",
    "    error = np.mean((np.abs(y_hats-test_y)))\n",
    "\n",
    "    # Accuracy is 1 minus the error\n",
    "    accuracy = 1-error\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test with data(text) react to \"政治\" category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cofact_label.csv')\n",
    "pos = df[df[\"政治\"] == 1]\n",
    "pos = pos[\"topic\"]\n",
    "pos = pos.values.tolist()\n",
    "pos = segment(pos)\n",
    "\n",
    "neg = df[df[\"政治\"] == 0]\n",
    "neg = neg[\"topic\"]\n",
    "neg = neg.values.tolist()\n",
    "neg = segment(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set)\n",
    "train_pos = pos[:int(len(pos)*0.8)]\n",
    "test_pos = pos[int(len(pos)*0.8):]\n",
    "train_neg = neg[:int(len(neg)*0.8)]\n",
    "test_neg = neg[int(len(neg)*0.8):]\n",
    "\n",
    "#Text\n",
    "train_x = train_pos + train_neg\n",
    "test_x = test_pos + test_neg\n",
    "\n",
    "#True label\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9808292530117266\n",
      "2654\n"
     ]
    }
   ],
   "source": [
    "freqs = count_tweets({}, train_x, train_y)\n",
    "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\\\n",
    "\n",
    "\n",
    "print(logprior)\n",
    "print(len(loglikelihood))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a source that is not related to \"政治\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: -0.9808292530117266\n",
      "一天到晚: 0\n",
      "台灣: 2.121879976496536\n",
      "中國: 1.0745609822159778\n",
      "打壓: 0\n",
      "聽到: 0\n",
      "政治: 0\n",
      "名嘴: 0\n",
      "口水戰: 0\n",
      "誤以為: 0\n",
      "台灣: 2.121879976496536\n",
      "真的: 1.1233511463854091\n",
      "競爭力: 0\n",
      "洗腦: 0\n",
      "台灣: 2.121879976496536\n",
      "國際: 2.7609599357862065\n",
      "間: -0.5348769302181227\n",
      "重要性: 0\n",
      "取代: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.808805810647353"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"一天到晚看著台灣被中國打壓，又聽到政治名嘴的口水戰，讓你誤以為台灣真的沒有競爭力？別再被洗腦啦！台灣在國際間的重要性，可是無人可取代的！\"\n",
    "w = segment([w])[0]\n",
    "naive_bayes_predict_detail(w, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The score(prob.) of text belong to that cate\n",
    "#Normally, if score > 0 mean that text belong to that cate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with a source that is not related to \"政治\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior: -0.9808292530117266\n",
      "新型: 0\n",
      "冠狀: -0.8225590026699043\n",
      "病毒: -2.87025184603516\n",
      "肆虐: 0\n",
      "全球: -0.12941182210995894\n",
      "確診: -3.3280849396606405\n",
      "病例: -1.045702553984114\n",
      "突破: 0\n",
      "1千萬: 0\n",
      "約: -1.228024110778068\n",
      "50萬: 1.6623476471180973\n",
      "死亡: 0\n",
      "美國: 1.5288162544935737\n",
      "約翰霍普金斯: 0\n",
      "大學: -1.5157061832298488\n",
      "統計: 0\n",
      "顯示: 0.969200466558152\n",
      "美國: 1.5288162544935737\n",
      "確診: -3.3280849396606405\n",
      "死亡: 0\n",
      "個案: -2.6551404664182146\n",
      "最多: 0\n",
      "佔: 0\n",
      "全球: -0.12941182210995894\n",
      "四分一: 0\n",
      "巴西: -0.12941182210995805\n",
      "感染: -2.144314842652223\n",
      "死亡: 0\n",
      "個案: -2.6551404664182146\n",
      "排: 0\n",
      "第二: -0.8225590026699043\n",
      "俄羅斯: 0\n",
      "排: 0\n",
      "第三: -1.382174790605327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-19.477627241460464"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"新型冠狀病毒肆虐，全球確診病例突破1千萬，約50萬人死亡。根據美國約翰霍普金斯大學的統計顯示，美國的確診及死亡個案最多，佔全球四分一；巴西的感染及死亡個案排第二，俄羅斯排第三。\"\n",
    "w = segment([w])[0]\n",
    "naive_bayes_predict_detail(w, logprior, loglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes accuracy = %0.4f\" %\n",
    "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tbh, the model give 100% acc. is mainly because the content(dataset) is highly duplicated and test size(20) is small.\n",
    "\n",
    "#### But at least this show that the model do the classification(categorization) in rational direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data:\n",
    "\n",
    "#### label: 1(True) if the text belong to the class(\"政治\"), else 0(False)\n",
    "#### Predict : prediction\n",
    "#### text: the text to predict(after segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1.0\n",
      "Predict: True\n",
      "Text: 台灣/價值/霸/罷/出征/世衛/秘書長/結束/出征/新加波/總理/夫人/台灣/真的/霸氣/發起/罷免/89萬/票/選出來/高雄/市長/高呼/罷免/世衛/秘書長/譚德塞/台灣/罷免/昌/逆/亡/意思/罷免/韓國瑜/貪污/市政/做/爛/內政部/市政/評比/前/二/名/罷免/罷免/譚德塞/好笑/根本/投票/投票/資格/喊/罷免/喊/大聲/先/整理/一下/新加波/總理/夫人/得罪/台灣/網民/始末/好了/ 1/25日/蘇貞昌/宣布/禁止/口罩/包含/新加波/ST/公司/台灣/兩/條/生產線/台灣/徵用/兩/條/產線/通融/新加坡/台灣/生產/口罩/運回/新加坡/ 2月/新加坡/口罩/缺貨/只好/台灣/新加坡/產線/撤回/新加坡/提供/新加坡人/使用/ 2/26/新加坡/總理/夫人/宣布產線/撤回/新加坡/ 4/7/台灣/宣布/捐/口罩/新加坡/總理/夫人/表示/台灣/網民/排山倒海/出征/新加坡/總理/夫人/謾罵/羞辱/譏諷/詛咒/滿滿/台灣/價值/種/國罵/紛紛/出籠/譚德塞/好歹/公開/記者會/指責/台灣/三/分鐘/總理/夫人/含蓄/臉書/寫/Err/台灣/網民/兩/國際/知名度/不/假/辭色/一視同仁/狂酸/怒罵/潑/糞/羞辱/台灣/走進/國際/社會/方式/台灣/價值/國格/蔡英文/真的/不/制止/一下/幼稚園/大學/教育/教導/溝通/兩/幼稚園/學生/爭奪/玩具/吵/時/老師/父母/引導/兩/小學生/誤會/大打出手/時/老師/父母/教育/兩/國中生/發生/討厭/霸凌/時/老師/父母/開導/從小到大/受/教育/教/歧視/羞辱/蠻橫/方式/解決/紛爭/先/誰是誰非/ 光/看看/網路/低級/下流/語言/真的/暈/兩/件/事/發生/字典/字典/命名為/台灣/罵人/詞彙/寶典/真的/種/方式/揚名/國際/島/嗨/好奇/國家/尊敬/中華民國/過去/台灣/世界/看見/早期/非洲/國家/幫忙/建設/農耕隊/踏足/災難/現場/悲憫/救世/慈濟人/助人/榮登/時代/雜誌/陳樹菊/阿嬤/棒球/好手/王建民/網球/王子/盧彥勳/電影界/奇才/李安/麵包/達人/吳寶春/科技業/巨擘/台積電/強國/台灣/彈丸之地/走到/世界/抬頭挺胸/才智/不/輸/世界/貢獻/有目共睹/曾經/驕傲/說/台灣/美/風景/現在/臉/自稱/美/風景/罵人/受/尊敬/用力/罵人/正義/罵/爽/賠掉/國格/國家/尊嚴/建立/國家/形象/需要/經年累月/付出/努力/毀滅/國家/形象/需要/兩三/天/意氣用事/介/無足輕重/草民/寫/文章/狗吠/火車/認同/通常/喪失/國格/群/相信/唯一/能力/制止/817萬/選出來/蔡英文/總統/網路/丟人現眼/酸民/不/國家/操控/網軍/多半/蔡/英文/總統/信徒/蔡/總統/留英正妹/加油/鼓勵/之外/不/控制/一下/群/網路/義和團/林薇/說詞/不/完全/認同/至少/禮貌/表達/譚德塞/先生/抗議/值得/肯定/貴/國家/元首/站/掌控/全/局/制高點/國家/前途/不/人民/蒸/口罩/省吃儉用/口罩/當成/鈔票/撒出去/光宗耀祖/國家/命運/生死/存亡/關鍵點/做/做/不/做/全/做/歷史會/記上/筆/認同/請/正/能量/群組/發出去\n",
      "label: 1.0\n",
      "Predict: True\n",
      "Text: 政府/現金/補助/領/ 1/保/國保/答案/份/ 2/保/農保/答案/份/ 3/健保/以外/保/答案/份/ 4/受僱/保/勞保/答案/份/放/薪假/解僱/公司/証明/衰退/嚴重/僱/主要/通報/申請/部份/補助/ 5/保/職業/公會/自營商/保/勞保/答案/需/投保/24000/以下/年/所得/40萬/以下/提出/申請/ 6/弱勢/家庭/答案/4500/元/戶/ㄧ次/ 7/保/勞保/失業/答案/份/ 8/7/種/以外/答案/份/領/轉貼\n",
      "label: 1.0\n",
      "Predict: True\n",
      "Text: 記得/今天/明天/2/天/家樂福/買/東西/順便/領取/1600/元/政府/補助/振興/卷切/記切/記不領/白不領\n",
      "label: 1.0\n",
      "Predict: True\n",
      "Text: 家樂福/振興券/ 商/噱/ 頭/ 1-/4/19 20/兩/天/家樂福/會員卡/須/購/元/商品/贈/1600/元/振興券/ 2-/券/4/22/5/10/日/家樂福/購/一千/貨品/即可/100/抵銷/900/元/1000/1999/元/100/元/券/抵銷/兩千/元/200/抵銷/類推/切記/使用/時間/18/天/算/一下/花/現金/買/貨物/18/天/用掉/1600/元/振興券\n",
      "label: 1.0\n",
      "Predict: True\n",
      "Text: 摘錄/胡蓉/ PO/文/台灣/大陸/防疫/一點/看出/優秀/民主/大陸/新聞/媒體/焦點/抗疫/線/醫護/人員/後勤/保障/人員/表現出/真實/面/大陸/防疫/幾十萬/英雄/上百萬/武漢人/焦點/政治/防疫/讓道/中國人/情不自禁/政府/點讚/政府/塑造/人民/優先/生命/第一/大陸/人民/疫情/團結/台灣/政治/優先/抹黑/大陸/優先/辱罵/WHO/優先/順時鐘/優先/名嘴/胡說八道/優先/台灣/真正/抗疫/普通/老百姓/聲音/聽/不/團結/大陸/撕裂/台灣/台灣/吹噓/健保卡/俗/不/大陸/新冠肺炎/患者/免費/輕/症/患者/治療/費用/3/5萬/重/症/患者/80多萬/110萬/台灣/吹噓/防疫/世界/第一/大陸/卯足/勁/復工/復產/台灣/吹噓/捐贈/1000萬/口罩/歐美/大陸/派出/幾十/醫療隊/支援/世界/援助/90多/國家/台灣/宣稱/民主/自由/幾百/武漢/台灣人/無法/回/台灣/大陸/世界/派出/幾十/專機/接/海外/中國人/大陸/口罩/隨處/買/大陸/健康碼/成為/出入/場所/通行證/大陸/政府/清楚/不足/大陸/政府/成績/滿足/看到/中國/大陸/政府/吹牛/低調/低/目標/高調/超額/完成/民進黨/風格/截然不同/民進黨/風格/先/說大話/最後/實現/實現/找/冠名堂皇/理由/忽悠/老百姓/大陸/新聞/聽到/大陸/政府/說/句/深刻/檢討/說明/大陸/政府/承認/錯誤/民進黨/沒完沒了/犯/錯誤/結論/台灣/30/年/前/GDP/佔/大陸/一半/30/年/大陸/不到/4%/全球/經濟/低迷/ECFA/會斷/估計/今年/台灣/2%/信誓/噹噹/說/台灣/民主/大陸/政府/體系/優越/恭喜/腦子/洗爆/民主/不/張/選票/民主/關鍵/政府/體系/政府/人民/著想/台灣/民主/張/選票/民粹/社會/驗證/大陸/位/清華/教授/句/民主/終點/必然/走向/香港/台灣/例子/看看/香港區/議員/選出/腦殘/看看/台灣/不/民主/成/台灣人/最後/塊/遮羞布/揭開/醜陋/不堪\n",
      "label: 1.0\n",
      "Predict: True\n",
      "Text: 今天/華爾街日報/世界/銀行/報告/全球/經濟/巳步/蕭條/階段/情況/1930年/蕭條/險陖/注意/全球/失業率/新冠/病毒/疫情/擴散/國/鎖國/造成/市場/需求/下降/產業/供應鍊/斷鍊/大量/企業/受/影響/結束/營業/倒閉/影響/失業/國/政府/大量/提供/救疫情/救/經濟/計劃/車水/杯薪/恐/難以/輓回/頹勢/建議/做好/準備/渡過/未來/兩/年/艱難/日子/輕言/放棄/不/做/適當/時機/撤出/巿場/保留/現金/市場/穏定/決定/投入/省/省/勉/有去無回/傷害/感情/荷包/貪得無厭/累積/堆/呆帳\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 確診/案例/屏東市/國民/運動/中心/ #/屏東/大學/ #/環球/影城/ #/潮州/健身/工廠/ #/長治鄉/玄天/上帝/附近/球場/屏東縣/新增/3/例/武漢/肺炎/確定/病例/皆/敦睦/艦隊案/人員/中央/流行/疫情/指揮/中心/公布/敦睦/艦隊/24/例/確診/19日/新增/案例/3/例/屏縣/個案/18日/1/例/4/例/皆/敦睦/艦隊/人員/確診/個案/目前/皆/收置/醫院/治療/中/屏東縣政府/接獲/新增/案件/立即/展開/項/緊急/防治/工作/嚴防/疫情/擴散/蔓延/衛生局/說明/4/名/個案/皆/4/15日/船/第1/例/當日/自行/騎車/家/1/小時/返回/軍艦/社區/活動史/第2/例/疫調/顯示/出入/公共/場所/屏東市/國民/運動/中心/4/16/17/30/19/00/4/17/15/50/17/30/長治鄉/玄天/上帝廟/附近/球場/4/17/17/30/19/00/地點/皆/短暫/戴/口罩/第3/例/4/16/下午/屏東/大學/找/朋友/4/17/屏東/大學/接/朋友/環球/影城/食用/晚餐/家/第4/例/潮州/健身/工廠/運動/4/16/10/30/14/00/日/全程/戴/口罩/搭乘/火車/高鐵/前往/縣市/疫調/資料/提供/中央/流行/疫情/指揮/中心/衛生局/初步/匡列/掌握/27/位/接觸/進行/居家/隔離/自主/健康/管理/進行/相關/衛教/環境/清消/4/20/相關/館場/封館/消毒/加強/監測/請/民眾/提高/警覺/相關/疫調/持續/進行/中/衛生局/表示/波/疫情/部分/確診/個案/社區/活動/提醒/民眾/出入/公共/場所/須/加強/防護/措施/防疫/重點/ 一/請/醫療/機構/提高/通報/警覺/加強/TOCC/通報/進行/必要/醫療/措施/ 二/強化/社區/防疫/民眾/近期/出現/發燒/呼吸道/症狀/嗅味覺/改變/疑似/症狀/請/配戴/口罩/儘速/就醫/衛生局/提醒/縣/防疫/安全/需要/縣民/共同/配合/項/防治/措施/疫情/傳播/風險/降到/低/再次/提醒/民眾/應/做好/手部/衛生/咳嗽/禮節/室/外/1/公尺/社交/安全/距離/無法/保持/請/配戴/口罩/搭乘/大眾/運輸/時/應/全程/配戴/口罩/配合/量測/體溫/身體/不適/請/戴/口罩/速/就醫/主動/告知/旅遊/接觸史/落實/生病/家/休息/加強/疑似/個案/診斷/通報/相關/防護/措施/避免/社區/感染/情事/發生/共同/維護/全民/健康/安全/ #/資訊/提供/屏東縣政府\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text:  is on Facebook/ To connect with /新/正薪/醫院/ join Facebook is on Facebook/ To connect with /新/正薪/醫院/ join Facebook today/❌❌❌ /嚴打假/消息/❌❌❌ /新/正薪/醫院/證實/假/消息/出現/網友/PO/出假/圖片/試圖/造謠/擾亂/防疫/ -------------------\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 嚇人/巴黎/非/飲用水/驗出/微量/武肺/病毒/緊急/停用/ #LINETODAY /巴黎/非/飲用水/驗出/微量/武肺/病毒/緊急/停用/ | /新/頭殼/ | LINE TODAY name /新/頭殼/ Reporter /新/頭殼/newtalk |/洪翠蓮/綜合/報導\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: || /美國/華裔/線/醫生/抗疫/硬核/心得/建議/梅奧/診所/醫療/中心/Mayo /Clinic/疫苗/研究/小組/負責人/波蘭/Gregory Poland/博士/說/分析/壹/種/新/疾病/壹/張/像素化/圖片/圖片/渲染/30％/時/無法/分辨/照片/內容/加載/70％/圖片/變/逐漸/清晰/波蘭/談到/新冠/病毒/時/說/認知/現在/30％/50％/之間/基因/角度/冠狀/病毒/解/臨床/流行病學/季節性/方面/種/冠狀病毒\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 挑/想要/金枕頭/榴槤/染色\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 確診/位/軍艦兵/週六/4/18/出遊/嘉義市/停留點/國軍/英雄/館/附近/四海/豆漿/夾/娃娃/機店/樂檸/中山店/林森東路/貓狗/用品店/對面/麵包店/蘭潭/風景區/最後/南下/間/中油/加油站/上廁所/士兵/嘉義市/停留/北上/時/停留/仁德/新營/休息站/上述/地點/請/兩/天/小心/避開\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 位/護士/摘掉/口罩/眼鏡罩/紫外線/顯出/臉/殘留/病毒/外出/回家/緊閉/眼睛/洗臉\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 台南/新增/敦睦/遠訓/病例/30餘/歲/男性/住/歸仁區/4/14/軍艦/左營/搭/計程車/回到/歸仁/居所/日/出門/晚上/倦怠感/4/15/出門/餐點/家人/準備/4/16/下午/位/友人/台南/新光三越/新天地/吃飯/逛街/個案/約/下午/ 4 點/新光三越/新天地/ B1 /搭/電梯/ 6樓/翰林/茶館/購買/飲料/直接/麥當勞/點餐/進入/公共客/席區/用餐/6點/離開/麥當勞/搭/ A 區/手扶梯/ 5 樓/逛/圈/入櫃/購物/搭/手/扶梯/西門路/大門/口/離開/全程/戴/口罩/交通/工具/自行/開車/回家/出門/4/17/家/中/出門/4/18/上午/9點/接獲/通知/中午/台南/公園/指定/地點/集合/晚間/6點/搭上/巴士/前往/檢疫所\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 緊急/通知/剛剛/接獲/市/衛生局/疾管處/專員/來電/表示/市/教師會/轉傳/1/份/磐石/軍艦/人員/走過/10/行政區/景點/訊息/假/消息/呼籲/轉傳/衛生局/啟動/偵查/開罰/請/看到/訊息/希望/轉傳/謝謝/妳\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 非常時期/印度/實施/國人/居家/隔離/不/準/外出/傷殘/人士/在外/國人/聽見/警車聲/扮/殘疾/人士/場面/搞笑/呲牙\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 超市/看到/種/大蒜/便宜/買/健康/知道/歡迎/收看/百科/太太/大蒜/烹調/美味/佳肴/過程/中/常用/調味品/大蒜/營養/價值/高/適量/吃/人體/健康/益處/大蒜/中/含/硒/元素/良好/抑制/癌瘤/抗/癌/作用/含有/具/殺菌力/大蒜素/具有/降/血脂/功效/大蒜/健康/吃錯/大蒜/人體/有害處/大蒜/營養/價值/比較/低/爲社麽/麽/說/現在/一起/看看/希望/次/買錯/健康/有害\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 吃完/鳳梨/碰/2/物/嚴重/危害/身體/99%/不/知道/吃完/鳳梨/碰/2/物/嚴重/危害/身體/99%/不/知道/ 欢/迎来/保健/养/生/吃/喝玩/乐/频道/们将会/提供/文字/语音/文章/希望/会/支持/给/予/鼓励/让/们为/带来/更多/精彩/视频/透過/影片/与/分享/点点滴滴/希望/喜欢 /保健/养生/吃/喝玩/乐/喜歡/作品/歡迎/分享/喔/~ /贊/種/鼓勵/分享/最好/支持/保健/养生/吃/喝玩/乐/分享/ 祝/永远/健康/活力/~ *\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 開車/朋友/請/ㄧ定/救/命/請/ㄧ定/救/命\n",
      "label: 0.0\n",
      "Predict: False\n",
      "Text: 喝/酒/有效/防疫/～ /案/379/酒店/女/公關/相關/接觸/共計/123/檢驗/昨日/確定/全數/陰性/ #/喝/酒/事/酒精/真的/殺菌/喔\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_x)):\n",
    "    print(f\"label: {test_y[i]}\")\n",
    "    print(f\"Predict: {naive_bayes_predict(test_x[i], logprior, loglikelihood) > 0}\")\n",
    "    print(f\"Text: {'/'.join(test_x[i])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem:\n",
    "\n",
    "1. Most of the data(text) irrelevant to Hong Kong -> prediction is good for cofact dataset does not mean it predict well for HK news/article\n",
    "    1. Need HK source(dataset) for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
